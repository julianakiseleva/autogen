{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pftZ-ZF1_BA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPUGFpKP1_BH"
      },
      "source": [
        "# Demonstrating the `AgentEval` framework using the task of solving GAIA bechmark\n",
        "\n",
        "This notebook aims to demonstrate how to `AgentEval` implemented through [AutoGen](https://github.com/microsoft/autogen) works, where we use a math problem-solving task as an example. \n",
        "`AgentEval` consists of two key components:\n",
        "\n",
        "- `CriticAgent`: This is an LLM-based agent that generates a list criteria $(c_1, \\dots, c_n)$ to help to evaluate a utility given task.\n",
        "\n",
        "- `QuantifierAgent`: This agent quantifies the performance of any sample task based on the criteria designed by the `CriticAgent` in the following way: $(c_1=a_1, \\dots, c_n=a_n)$\n",
        "\n",
        "![AgentEval](../website/blog/2023-11-11-AgentEval/img/agenteval-CQ.png)\n",
        "\n",
        "For more detailed explanations, please refer to the accompanying [blog post](https://https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval)\n",
        "\n",
        "## Requirements\n",
        "\n",
        "AutoGen requires `Python>=3.8`. To run this notebook example, please install pyautogen, Docker, and OpenAI:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
          "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
          "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
          "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
        },
        "id": "68lTZZyJ1_BI",
        "outputId": "15a55fab-e13a-4654-b8cb-ae117478d6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyautogen~=0.2.0b5 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.9)\n",
            "Requirement already satisfied: docker in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (6.1.3)\n",
            "Requirement already satisfied: flaml in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (2.1.1)\n",
            "Requirement already satisfied: diskcache in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (5.6.3)\n",
            "Requirement already satisfied: termcolor in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (2.3.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (0.5.1)\n",
            "Requirement already satisfied: openai>=1.3 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (1.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (2.5.0)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pyautogen~=0.2.0b5) (1.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from docker) (0.58.0)\n",
            "Requirement already satisfied: pywin32>=304 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from docker) (305.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from docker) (1.26.14)\n",
            "Requirement already satisfied: packaging>=14.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from docker) (22.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from docker) (2.28.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (4.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (0.25.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (4.64.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (3.5.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from openai>=1.3->pyautogen~=0.2.0b5) (1.8.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.1 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10->pyautogen~=0.2.0b5) (2.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10->pyautogen~=0.2.0b5) (0.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->docker) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->docker) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.26.0->docker) (2022.12.7)\n",
            "Requirement already satisfied: six in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from websocket-client>=0.32.0->docker) (1.16.0)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from flaml->pyautogen~=0.2.0b5) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from tiktoken->pyautogen~=0.2.0b5) (2022.7.9)\n",
            "Requirement already satisfied: httpcore in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen~=0.2.0b5) (1.0.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>4->openai>=1.3->pyautogen~=0.2.0b5) (0.4.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from httpcore->httpx<1,>=0.23.0->openai>=1.3->pyautogen~=0.2.0b5) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from scipy) (1.23.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib) (22.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jukisele\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyautogen~=0.2.0b5 docker\n",
        "%pip install scipy\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxgqKJrd1_BJ"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "* The [`config_list_openai_aoai`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_openai_aoai) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
        "  - OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
        "  - Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
        "  - Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
        "* The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file. It first looks for an environment variable with a specified name. The value of the environment variable needs to be a valid json string. If that variable is not found, it looks for a json file with the same name. It filters the configs by filter_dict.\n",
        "\n",
        "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YRycFEDJ1_BJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
            "  \"class\": algorithms.Blowfish,\n"
          ]
        }
      ],
      "source": [
        "import autogen\n",
        "import json\n",
        "\n",
        "#config_list = json.loads(secrect-string)\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4\"],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBZ-XFXy1_BJ"
      },
      "source": [
        "\n",
        "## Construct `CriticAgent`\n",
        "\n",
        "We construct the planning agent named `critic` and a user proxy agent for the critic named `critic_user`. We specify `human_input_mode` as \"NEVER\" in the user proxy agent, ensuring that it will never ask for human feedback. Additionally, we define the `ask_critic` function to send a message to the critic and retrieve the criteria from the critic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9XAeyjd11_BK"
      },
      "outputs": [],
      "source": [
        "\n",
        "critic = autogen.AssistantAgent(\n",
        "    name = \"critic\",\n",
        "    llm_config = {\"config_list\": config_list, \"max_retries\": 10},\n",
        "    system_message = \"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be dinstinguishable, quantifieable and not redundant.\n",
        "    Convert the evaluation criteria into a dictionary where the keys are the criteria.\n",
        "    The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key}\n",
        "    Make sure the keys are criteria for assessing the given task.  \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferrably mlti-graded levels. \"description\" includes the criterion description.\n",
        "    Return the dictionary.\"\"\"\n",
        ")\n",
        "\n",
        "critic_user = autogen.UserProxyAgent(\n",
        "    name = \"critic_user\",\n",
        "    max_consecutive_auto_reply = 0,  # terminate without auto-reply\n",
        "    human_input_mode = \"NEVER\",\n",
        "    code_execution_config={\"use_docker\": False},\n",
        ")\n",
        "\n",
        "def ask_critic(message):\n",
        "    \"\"\"\n",
        "    Initiate a chat with the critic user and return the last message received from the planner.\n",
        "\n",
        "    Args:\n",
        "    - message (str): The message to be sent to the critic user.\n",
        "\n",
        "    Returns:\n",
        "    - str: The content of the last message received.\n",
        "    \"\"\"\n",
        "    critic_user.initiate_chat(critic, message=message)\n",
        "    # return the last received from the planner\n",
        "    return critic_user.messagelast_message()[\"content\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPTtNkhk2V1"
      },
      "source": [
        "# Run the Critic\n",
        "\n",
        "To run the critic, we need a couple of math problem examples. One of them failed to solve the problem successfully, given in `agenteval-in-out/response_failed.txt`, and the other one was solved successfully, i.e., `agenteval-in-out/response_successful.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5H1WRs_wkiK0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Reading one successful and one failed example of the task\n",
        "response_successful = open(\"../logs/gaia_1output_equal.jsonl\",\"r\").readlines()\n",
        "response_failed = open(\"../logs/gaia_1output_not_equal.jsonl\",\"r\").readlines()\n",
        "\n",
        "task = {\"name\": \"GAIA\",\n",
        "      \"description\": \"The task is answer the Real-world and challenging question provided in the field Question. The proposed solution is generated by multi-agent system. The archive the goal the agent needs to do number of steps. Answering the questions requires successful completion of some number of steps, which cannot easily be brute forced due to their diversity. The possibility to check the reasoning trace, the accuracy required in the answers, their absence in plain text from the internet prevent a possible data contamination. \",\n",
        "      \"successful_response\" : response_successful,\n",
        "      \"failed_response\" : response_failed}\n",
        "\n",
        "sys_msg = f\"\"\"Task: {task[\"name\"]}.\n",
        "Task description: {task[\"description\"]}\n",
        "Task successfull example: {task[\"successful_response\"]}\n",
        "Task failed example: {task[\"failed_response\"]}\n",
        "\"\"\"\n",
        "\n",
        "current_task_name = '_'.join(task[\"name\"].split()).lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu70o024lenI"
      },
      "source": [
        "# The Criteria\n",
        "Now, we print the designed criteria. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9DsDB5hqvtG",
        "outputId": "0edd7a0c-b031-4f67-efc6-1a1e77066921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mcritic_user\u001b[0m (to critic):\n",
            "\n",
            "Task: GAIA.\n",
            "Task description: The task is answer the Real-world and challenging question provided in the field Question. The proposed solution is generated by multi-agent system. The archive the goal the agent needs to do number of steps. Answering the questions requires successful completion of some number of steps, which cannot easily be brute forced due to their diversity. The possibility to check the reasoning trace, the accuracy required in the answers, their absence in plain text from the internet prevent a possible data contamination. \n",
            "Task successfull example: []\n",
            "Task failed example: []\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "APIStatusError",
          "evalue": "api-key expired; please login to website",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen_criteria \u001b[38;5;241m=\u001b[39m \u001b[43mcritic_user\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys_msg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m128000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m criteria \u001b[38;5;241m=\u001b[39m critic_user\u001b[38;5;241m.\u001b[39mlast_message()\n\u001b[0;32m      3\u001b[0m cr_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../test/test_files/agenteval-in-out/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_task_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_criteria.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:698\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, cache, **context)\u001b[0m\n\u001b[0;32m    696\u001b[0m     agent\u001b[38;5;241m.\u001b[39mclient_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 698\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n\u001b[0;32m    700\u001b[0m     agent\u001b[38;5;241m.\u001b[39mclient_cache \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mprevious_cache\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:441\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    439\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:599\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1298\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m-> 1298\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:813\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    810\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m--> 813\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# ensure function and tool calls will be accepted when sent back to the LLM\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\oai\\client.py:283\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[1;34m(self, **config)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# filter is not passed; try the next config\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    285\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\autogen\\oai\\client.py:548\u001b[0m, in \u001b[0;36mOpenAIWrapper._completions_create\u001b[1;34m(self, client, params)\u001b[0m\n\u001b[0;32m    546\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    547\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     response \u001b[38;5;241m=\u001b[39m completions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\openai\\_utils\\_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\openai\\resources\\chat\\completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\openai\\_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[1;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\openai\\_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jukisele\\AppData\\Local\\anaconda3\\lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    959\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    963\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    964\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    968\u001b[0m )\n",
            "\u001b[1;31mAPIStatusError\u001b[0m: api-key expired; please login to website"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_criteria = critic_user.initiate_chat(critic, message=sys_msg[:128000])\n",
        "criteria = critic_user.last_message()\n",
        "cr_file = open(f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\",\"w\")\n",
        "cr_file.write(criteria[\"content\"])\n",
        "cr_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PETPZluOEGCR"
      },
      "source": [
        "*Note :* You can also define and use your own criteria by editing `criteria.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmpUZv_ylo9U"
      },
      "source": [
        "# The `QuantifierAgent`\n",
        "\n",
        "Once we have the criteria, we need to quantify a new sample based on the designed criteria and its accepted values. This will be done through `QuantifierAgent` agent as follows. \n",
        "We note that can skip the designed creteria by the agent and use your own defined criteria in `criteria_file`. Check the file before going to the next step making sure it's contains only dict, it may produce something extra text sometimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4uUkZJh_subA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "criteria_file = f\"../test/test_files/agenteval-in-out/{current_task_name}_criteria.json\"\n",
        "\n",
        "quantifier = autogen.AssistantAgent(\n",
        "    name = \"quantifier\",\n",
        "    llm_config = {\"config_list\": config_list, \"max_retries\": 10},\n",
        "    system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria.\n",
        "    The criterion is given in a dictionary format where each key is a dintinct criteria.\n",
        "    The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key}\n",
        "    You are going to quantify each of the crieria for a given task based on the task decription.\n",
        "    Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.\n",
        "    Return only the dictionary.\"\"\"\n",
        ")\n",
        "\n",
        "quantifier_user = autogen.UserProxyAgent(\n",
        "    name = \"quantifier_user\",\n",
        "    max_consecutive_auto_reply = 0,  # terminate without auto-reply\n",
        "    human_input_mode = \"NEVER\",\n",
        "    code_execution_config={\"use_docker\": False},\n",
        ")\n",
        "\n",
        "dictionary_for_eval = open(criteria_file,\"r\").read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64rRJfB2l6lO"
      },
      "source": [
        "## Running the quantifier on a GAIA Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ0H3sy8l-Ai"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def read_gaia_logs(file_name, correctness):\n",
        "    \"\"\"\n",
        "    Read the mathproblem logs line by line - extract specific fields.\n",
        "\n",
        "    Args:\n",
        "    - file_name (str): The single log file that wants to get evaluated.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of tuples, each containing the test case and correctness.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    with open(file_name, \"r\") as f:\n",
        "        for line in f:\n",
        "            #print(\"Line: \"+ line)\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                task_id = data.get('task_id','')\n",
        "                question = data.get('Question', '')\n",
        "                reasoning_trace = json.dumps(data.get('reasoning_trace', ''))\n",
        "                test_case = \"Question is \"+ question + \" The agent reasoning is \" + reasoning_trace\n",
        "                results.append((task_id, test_case[:128000], correctness))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_quantifier(success_file, failed_file, criteria_file):\n",
        "    \"\"\"\n",
        "    Running quantifier agent on individual log.\n",
        "\n",
        "    Args:\n",
        "    - success_file (str): The log path for successful cases.\n",
        "    - failed_file (str): The log path for failed cases.\n",
        "    - criteria_file (str): The criteria JSON file path.\n",
        "    \n",
        "    Returns:\n",
        "    - dict: A dictionary including the actual success of each problem as well as estimated performance by the agent eval.\n",
        "      {\"results\": [{\"actual_success\": actual_label, \"estimated_performance\": quantified_results[\"content\"]}, ...]}\n",
        "    \"\"\"\n",
        "    dictionary_for_eval = open(criteria_file, \"r\").read()\n",
        "    results = []\n",
        "\n",
        "    # Append results for successful and failed cases\n",
        "    results.extend(read_gaia_logs(success_file, \"correct\"))\n",
        "    results.extend(read_gaia_logs(failed_file, \"incorrect\"))\n",
        "  \n",
        "    \n",
        "    output_file_path = \"../test/test_files/agenteval-in-out/gaia_evaluated_problems.json\"\n",
        "    output_file_path_line_by_line = \"../test/test_files/agenteval-in-out/per_line_gaia_evaluated_problems.jsonl\"\n",
        " \n",
        "    all_data = {}\n",
        "\n",
        "    # Iterate through the loop where you have access to task_id, test_case, and actual_label\n",
        "    for task_id, test_case, actual_label in results:\n",
        "        quantifier_user.initiate_chat(quantifier, message=sys_msg + \\\n",
        "                                                \"Evaluation dictionary: \" + str(dictionary_for_eval) + \\\n",
        "                                                \"Actual test case to evaluate: \" + test_case)\n",
        "        quantified_results = quantifier_user.last_message()\n",
        "\n",
        "        # Construct the nested dictionary for each file\n",
        "        nested_dict = {\n",
        "            \"task_id\": task_id,\n",
        "            \"actual_success\": actual_label,\n",
        "            \"estimated_performance\": quantified_results[\"content\"]\n",
        "        }\n",
        "\n",
        "        # Add the nested dictionary to the all_data dictionary\n",
        "        all_data[task_id] = nested_dict\n",
        "\n",
        "        # Write each line of the nested dictionary to the JSONL file\n",
        "        with open(output_file_path_line_by_line, \"a\") as file:\n",
        "            json.dump(nested_dict, file)\n",
        "            file.write('\\n')  # Add a newline after each JSON object\n",
        "\n",
        "\n",
        "    # Write the entire dictionary to the JSON file\n",
        "    with open(output_file_path, \"w\") as file:\n",
        "        json.dump(all_data, file, indent=2)\n",
        "\n",
        "\n",
        "get_quantifier(\"../test/test_files/agenteval-in-out/sample-gaia_1output_equal.jsonl\", \"../test/test_files/agenteval-in-out/sample-gaia_1output_not_equal.jsonl\", criteria_file)  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
