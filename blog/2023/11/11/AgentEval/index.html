<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">AgentEval | AutoGen</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="AgentEval | AutoGen"><meta data-react-helmet="true" name="description" content="---"><meta data-react-helmet="true" property="og:description" content="---"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2023-11-11T00:00:00.000Z"><link data-react-helmet="true" rel="shortcut icon" href="/autogen/img/ag.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/2023/11/11/AgentEval" hreflang="x-default"><link rel="stylesheet" href="/autogen/assets/css/styles.a35b243d.css">
<link rel="preload" href="/autogen/assets/js/runtime~main.606e2dd2.js" as="script">
<link rel="preload" href="/autogen/assets/js/main.adb8d8ff.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">AutoGen</b></a><a class="navbar__item navbar__link" href="/autogen/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/autogen/docs/reference/agentchat/conversable_agent">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/autogen/blog">Blog</a><a class="navbar__item navbar__link" href="/autogen/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/13/OAI-assistants">AutoGen Meets GPTs</a></li><li class="sidebarItem_cjdF"><a aria-current="page" class="sidebarItemLink_zyXk sidebarItemLinkActive_wcJs" href="/autogen/blog/2023/11/11/AgentEval">AgentEval</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/09/EcoAssistant">EcoAssistant - Using LLM Assistants More Accurately and Affordably</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/10/26/TeachableAgent">AutoGen&#x27;s TeachableAgent</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_d4p0" itemprop="headline">AgentEval</h1><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-11-11T00:00:00.000Z" itemprop="datePublished">November 11, 2023</time> Â· <!-- -->9 min read</div></header><div class="markdown" itemprop="articleBody"><hr><p>title:  AgentEval: Assessing Task Utility for LLM-powered Applications
authors: julianakiseleva, Narabzad</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="tags-llm-gpt-evaluation-task-utility">tags: <!-- -->[LLM, GPT, evaluation, task utility]<a aria-hidden="true" class="hash-link" href="#tags-llm-gpt-evaluation-task-utility" title="Direct link to heading">â€‹</a></h2><p><img alt="Fig.1: A verification framework" src="/autogen/assets/images/agenteval-CQ-de78a4d376a80e6429a082374cf424ae.png"></p><p><strong>TL;DR:</strong></p><ul><li>We introduce <code>AgentEval</code> â€” the first version of the framework to automatically assess task utility for an arbitrary application. It suggests criteria to explain task utility and then quantifies these criteria for logs of your system.</li><li>We demonstrate the usage of our framework with the <a href="https://microsoft.github.io/autogen/blog/2023/06/28/MathChat" target="_blank" rel="noopener noreferrer">Math Problems dataset</a></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="run-it-yourself">Run it yourself<a aria-hidden="true" class="hash-link" href="#run-it-yourself" title="Direct link to heading">â€‹</a></h2><p>Use the Jupyter notebook for a demonstration of <code>AgentEval</code> for <a href="https://microsoft.github.io/autogen/blog/2023/06/28/MathChat" target="_blank" rel="noopener noreferrer">Math Problems dataset</a> -- <a href="https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb" target="_blank" rel="noopener noreferrer">agenteval_cq_math.ipynb</a></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="introduction">Introduction<a aria-hidden="true" class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h2><p>The goal of AutoGen is to make it straightforward and easy to develop LLM-based multi-agent systems for an arbitrary application that helps the end users with their tasks. The next big step forward is what we all want to know: how the developed systems are performing, if they bring utility to our users, and, even more importantly, how we can improve them. Directly evaluating the multi-agent systems is tricky since current approaches mainly rely on success metrics, basically if the agent can accomplish the tasks. Moreover, the definition of success for a particular task is not always given. Introductions of LLMs bring us a lot of abilities that we are eager to convert into actual utilities for end users. In this blog post, we introduce <code>AgentEval</code> â€” the first version of the framework for assessing task utility in LLM-based applications.</p><p><img alt="Fig.2: An overview of the tasks taxonomy" src="/autogen/assets/images/tasks-taxonomy-af53dd06e0ae44ced809fc4cd10d5196.png"></p><p>Let&#x27;s first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, namely:</p><ul><li><em>Assistive Tasks</em> - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.</li><li><em>Executable Tasks</em> -refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:<ul><li><em>The optimal solution exits</em> -  there are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.</li><li><em>Multiple solutions exist</em> - &quot;Increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.</li></ul></li></ul><p>In our AgentEval framework, we are currently focusing on  <em>Executable Tasks</em>.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="agenteval-framework">AgentEval Framework<a aria-hidden="true" class="hash-link" href="#agenteval-framework" title="Direct link to heading">â€‹</a></h2><p>Our previous research on <a href="https://github.com/microsoft/iglu-datasets" target="_blank" rel="noopener noreferrer">assistive agents in Minecraft</a> suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. Interestingly, according to our earlier work, humans can develop criteria to explain why they prefer one agent over another. For example, &#x27;the first agent was faster in execution,&#x27; or &#x27;the second agent moves more naturally.&#x27; With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess the task <em>utility</em> for the multi-agent system. Namely:</p><ul><li>The goal of <code>CriticAgent</code> is to suggest the list of criteria $(c_1,\dots, c_n)$, that can be used to assess task utility. This is an example of how <code>CriticAgent</code> is defined using <code>Autogen</code>:</li></ul><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">critic </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;critic&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;config_list&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Convert the evaluation criteria into a dictionary where the keys are the criteria.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    The value of each key is a dictionary as follows {&quot;description&quot;: criteria description, &quot;accepted_values&quot;: possible accepted inputs for this key}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Make sure the keys are criteria for assessing the given task. &quot;accepted_values&quot; include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. &quot;description&quot; includes the criterion description.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Return only the dictionary.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ul><li>The goal of <code>QuantifierAgent</code> is to quantify each of the suggested criteria in the following way: $(c_1=a_1, \dots, c_n=a_n)$, providing us with an idea of the utility of this system for the given task -- $U(t_i)$. Here is an example of how it can be defined:</li></ul><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">quantifier </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;quantifier&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;config_list&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are a helpful assistant. You quantify the output of different tasks based on the given criteria.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    The criterion is given in a dictionary format where each key is a distinct criteria.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    The value of each key is a dictionary as follows {&quot;description&quot;: criteria description , &quot;accepted_values&quot;: possible accepted inputs for this key}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    You are going to quantify each of the criteria for a given task based on the task description.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Return only the dictionary.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="agenteval-results-based-on-math-problems-dataset"><code>AgentEval</code> Results based on math problems dataset<a aria-hidden="true" class="hash-link" href="#agenteval-results-based-on-math-problems-dataset" title="Direct link to heading">â€‹</a></h2><p> As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:</p><table><thead><tr><th>Criteria</th><th>Description</th><th>Accepted Values</th></tr></thead><tbody><tr><td>Problem Interpretation</td><td>Ability to correctly interpret the problem</td><td>[&quot;completely off&quot;, &quot;slightly relevant&quot;, &quot;relevant&quot;, &quot;mostly accurate&quot;, &quot;completely accurate&quot;]</td></tr><tr><td>Mathematical Methodology</td><td>Adequacy of the chosen mathematical or algorithmic methodology for the question</td><td>[&quot;inappropriate&quot;, &quot;barely adequate&quot;, &quot;adequate&quot;, &quot;mostly effective&quot;, &quot;completely effective&quot;]</td></tr><tr><td>Calculation Correctness</td><td>Accuracy of calculations made and solutions given</td><td>[&quot;completely incorrect&quot;, &quot;mostly incorrect&quot;, &quot;neither&quot;, &quot;mostly correct&quot;, &quot;completely correct&quot;]</td></tr><tr><td>Explanation Clarity</td><td>Clarity and comprehensibility of explanations, including language use and structure</td><td>[&quot;not at all clear&quot;, &quot;slightly clear&quot;, &quot;moderately clear&quot;, &quot;very clear&quot;, &quot;completely clear&quot;]</td></tr><tr><td>Code Efficiency</td><td>Quality of code in terms of efficiency and elegance</td><td>[&quot;not at all efficient&quot;, &quot;slightly efficient&quot;, &quot;moderately efficient&quot;, &quot;very efficient&quot;, &quot;extremely efficient&quot;]</td></tr><tr><td>Code Correctness</td><td>Correctness of the provided code</td><td>[&quot;completely incorrect&quot;, &quot;mostly incorrect&quot;, &quot;partly correct&quot;, &quot;mostly correct&quot;, &quot;completely correct&quot;]</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p>Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:</p><ul><li>AgentChat</li><li>ReAct</li><li>GPT-4 Vanilla Solver</li></ul><p>Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.</p><p><img alt="Fig.3: Results based on overall mathproblems datase `_s` stands for successful cases, `_f` - stands for failed cases" src="/autogen/assets/images/math-problems-plot-03ec81b957c85db6ad9b1da353784b96.png"></p><p>We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Sutogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to more accurately estimate the performance of successful cases than that of failed ones. This observation verifies AgentEval&#x27;s ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.</p><p>It&#x27;s important not only to identify what is not working but also to recognize what and why actually went well.For example, Explanation Clarity turned out to be correlated with the length of the solution and  all models master <em>Task Understanding</em> criteria.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="limitations-and-future-work">Limitations and future work<a aria-hidden="true" class="hash-link" href="#limitations-and-future-work" title="Direct link to heading">â€‹</a></h2><p>The current implementation of <code>AgentEval</code> has a number of limitations which are planning to overcome in the future:</p><ul><li>The list of criteria varies per run (unless you store a seed). We would recommend to run <code>CriticAgent</code> at least two times, and pick criteria you think is important for your domain.</li><li>The results of the <code>QuantifierAgent</code> can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.</li></ul><p>To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="summary">Summary<a aria-hidden="true" class="hash-link" href="#summary" title="Direct link to heading">â€‹</a></h2><p><code>CriticAgent</code> and <code>QuantifierAgent</code> can be applied to the logs of any type of application, giving you an in-depth understanding of the utility your model brings to the user.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="previous-research">Previous Research<a aria-hidden="true" class="hash-link" href="#previous-research" title="Direct link to heading">â€‹</a></h2><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@InProceedings{pmlr-v176-kiseleva22a,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  title = &quot;Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  author = &quot;Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\^o}t{\&#x27;e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  booktitle = &quot;Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  pages = &quot;146--161&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  year = 2022,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  editor = &quot;Kiela, Douwe and Ciccone, Marco and Caputo, Barbara&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  volume = 176,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  series = &quot;Proceedings of Machine Learning Research&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  month = &quot;06--14 Dec&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  publisher = &quot;PMLR&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  pdf =      {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  url =      {https://proceedings.mlr.press/v176/kiseleva22a.html}.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@InProceedings{pmlr-v220-kiseleva22a,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  title = &quot;Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  author = &quot;Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\^{o}t\&#x27;e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  booktitle = &quot;Proceedings of the NeurIPS 2022 Competitions Track&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  pages = &quot;204--216&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  year = 2022,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  editor = &quot;Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  volume = 220,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  series = &quot;Proceedings of Machine Learning Research&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  month = &quot;28 Nov--09 Dec&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  publisher = &quot;PMLR&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  pdf = &quot;https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  url = &quot;https://proceedings.mlr.press/v220/kiseleva22a.html&quot;.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/autogen/blog/2023/11/13/OAI-assistants"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Â« <!-- -->AutoGen Meets GPTs</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/autogen/blog/2023/11/09/EcoAssistant"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">EcoAssistant - Using LLM Assistants More Accurately and Affordably<!-- --> Â»</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#tags-llm-gpt-evaluation-task-utility" class="table-of-contents__link toc-highlight">tags: LLM, GPT, evaluation, task utility</a></li><li><a href="#run-it-yourself" class="table-of-contents__link toc-highlight">Run it yourself</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#agenteval-framework" class="table-of-contents__link toc-highlight">AgentEval Framework</a></li><li><a href="#agenteval-results-based-on-math-problems-dataset" class="table-of-contents__link toc-highlight"><code>AgentEval</code> Results based on math problems dataset</a></li><li><a href="#limitations-and-future-work" class="table-of-contents__link toc-highlight">Limitations and future work</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#previous-research" class="table-of-contents__link toc-highlight">Previous Research</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 AutoGen Authors.</div></div></div></footer></div>
<script src="/autogen/assets/js/runtime~main.606e2dd2.js"></script>
<script src="/autogen/assets/js/main.adb8d8ff.js"></script>
</body>
</html>